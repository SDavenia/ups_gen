# ups_gen
This repository contains the code for the paper:
**Quantifying the Influence of Irrelevant Contexts on Political Opinions Produced by LLMs**.

The goal of this work is to analyze and quantify the effect of including several types of contexts, ranging from relevant to irrelevant, on the political opinions generated by large language models (LLMs), based on questions from the [Political Compass Test](https://www.politicalcompass.org/) (PCT).


## Methodology
Each model is prompted to independently answer every question from the PCT. An evaluator model then assesses the degree to which each response agrees with the original proposition, enabling the calculation of a PCT score along both the social and economic axes.

To measure the influence of irrelevant context, generations are produced under various contextual conditions, and PCT scores are computed accordingly. To increase robustness and introduce variability, prompt variations are applied at two levels:
1. Question Templates ‚Äì Different formulations are used to prompt the model's opinion (e.g., "You are writing a blog post on <proposition>" vs. "You are at a political rally and must state your opinion on <proposition>").

2. Jailbreak Options ‚Äì Variations are introduced in the phrasing to encourage the model to take a clear and assertive stance on the given proposition.

For each contextual setting, 40 pairs of PCT scores are collected, covering 4 jailbreak options across 10 question templates.

## ‚öôÔ∏è How to run the Pipeline
The code to obtain the generations and the evaluations is briefly described below.

### 1. `generate_answers.py`

Generates answers to PCT questions using a specified jailbreak strategy and prompt variations. Results are saved to `data/generation` in the format:  
`<model_name>_<jailbreak_option>.csv`

**Usage:**
```
python generate_answers.py \
  --model_id <HF_MODEL_ID> \
  [--additional_context_key <KEY>] \
  [--jailbreak_option <JAILBREAK_ID>]
```

Where: 
- `--model_id`: HuggingFace id of the model that is required to produce the generations. 
- `--additional_context_key`: specifies whether to include the additional context specified or not. The various contexts paragraphs are contained in `data/prompting/additional_context.json`. If left empty, the base case without any additional context is used. 
- `--jailreak_option`: specifies which jail-break option to include. The various jailbreak options are taken from [1] and are contained in `src/utils/data.py`.

### 2. `wright_open_to_close.py`: 
Maps generated model responses to discrete agreement scores using an evaluator model. Adds a decision column to the input CSV and stores the result in data/generation_processed. By default (as in [2]), the evaluator is `Mistral-Instruct-7B-v0.3`.

**Usage**:
```
python wright_open_to_close.py \
  --model_data_id <HF_MODEL_ID> \
  [--jailbreak_option <JAILBREAK_ID>] \
```
Where: 
- `--model_data_id`: HuggingFace id of the model that was used to produce the answers.
- `--jailreak_option`: specifies which jail-break option to include.


### 3. `map_to_pct_axis.py`
Given a specific model used to process the data, it takes all files in the directory `data/generation_processed` for that model and computes the PCT score for both economic and social axes for each specific (`additional_context`, `prompt_template`, `jailbreak_option`) tuple and saves them in `data/results_pct/pct_results.csv`. Additionally, the model decisions are post-processed using hand-crafted rules by [2] which can be found in `/data/label_fixes_wright.json`.

**Usage**:
```
python map_to_pct_axis.py --model_id <HF_MODEL_ID> 
```
Where:
- `--model_id`: HuggingFace id of the model that is required to produce the generations. 


## üìä Plotting & Analysis
Use the following notebooks for visualization and statistical analysis:
- `visualize_pct_shifts.ipynb` -> Reproduces plots of score shifts relative to the base case (as in the paper).
- `visualize_fit_lmm.ipynb` -> Fits linear mixed models for analyzing RQ1 and RQ2.
 
## üìÅ Structure of the Repository
- `src/`  
  - `generate_answers.py` ‚Äì Script for generating answers  
  - `wright_open_to_close.py` ‚Äì Script for obtaining the evaluator model decisions.
  - `map_to_pct_axis.py` ‚Äì Maps decision to PCT.
  - `visualize_pct_shifts.ipynb` ‚Äì Contains code to obtain the plots used in the paper.
  - `visualize_fit_lmm.ipynb` ‚Äì Contains code to obtain the statistical analysis contained in the paper.
  - `utils/`  
    - `data.py` ‚Äì Contains functions to create the inputs to the models.
    - `run.py` ‚Äì Contains functions to obtain the model generations.
    - `utils.py` ‚Äì Contains shared utility functions used throughout the project. 
  - `data/`
    - `label_fixes_wright.json` ‚Äì Contains the manually crafted fixes for the labels generated by the evaluator model from [2]. 
    - `political_compass/`
      - `political_compass_questions.txt` ‚Äì Contains the full set of PCT questions.
    - `prompting/`
      - `additional_context.json` ‚Äì Contains the Wikipedia paragraphs extracted to be used as additional context.
      - `prompts_wright.json` ‚Äì Contains the generation templates which are included in the model input.
      - `jailbreak_options_rottger.json` ‚Äì Contains the jailbreak options which are included in the model input.
      - `generation_args_wright.json` ‚Äì Contains the args passed to the model for generation (same as [2]).
      - `evaluation_args_wright.json` ‚Äì Contains the args passed to the evaluator model for making the decision (same as [2]).


## References
- [1] Paul R√∂ttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Kirk, Hinrich Sch√ºtze, and Dirk Hovy. (2024). **Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.** *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 15295‚Äì15311, Bangkok, Thailand.

- [2] Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, and Isabelle Augenstein. (2024). **LLM Tropes: Revealing Fine-Grained Values and Opinions in Large Language Models.**  *Findings of the Association for Computational Linguistics: EMNLP 2024*, pp. 17085‚Äì17112, Miami, Florida, USA.  Association for Computational Linguistics.
