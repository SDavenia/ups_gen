import pathlib
import os
import re
import argparse
import logging

import pandas as pd

from copy import deepcopy as dp
from utils.utils import prepare_logger, fix_label, read_json
from utils.plotting_utils import plot_shaded_bars, create_context_placement_grid
from utils.plotting_utils import plot_context_placement

# Additional global variables that may be needed
VALID_INVALID_MAP = {"valid": 1, "invalid": 0}
ANSWER_MAP = {
    "Strongly disagree": 0,
    "Strongly Disagree": 0,
    "Disagree": 1,
    "disagree": 1,
    "Agree": 2,
    "agree": 2,
    "Strongly agree": 3,
    "Strongly Agree": 3,
    "None": -1,
}
ANSWER_MAP_PLOTTING = ANSWER_MAP.copy()  # None has to be in between
ANSWER_MAP_PLOTTING["None"] = 1.5
ADDITIONAL_CONTEXT_KEY_CATEGORIES = ['wiki_mus', 'wiki_obj', 'wiki_pol']

COLOR_MAP = {
    "Strongly Disagree": "#8B0000",  # Dark Red (DarkRed)
    "Disagree": "#FF6347",  # Light Red (Tomato)
    "None": "#AAAAAA",  # Gray
    "Agree": "#90EE90",  # Light Gree
    "Strongly Agree": "#006400",  # Dark Green (DarkGreen)
}

def parse_command_line_args():
    parser = argparse.ArgumentParser()
    # General directories to read dataframes from: should be left to default structure of the directory.
    parser.add_argument(
        "--processed_generated_answers_dir",
        type=pathlib.Path,
        default=pathlib.Path("../data/generation_processed/"),
        help="Directory containing the answers generated by the model",
    )
    parser.add_argument(
        "--pct_results_dir",
        type=pathlib.Path,
        default=pathlib.Path("../data/results_pct/"),
    )
    # General directories to write dataframes to
    parser.add_argument(
        "--output_plots_generated_answers_dir",
        type=pathlib.Path,
        default=pathlib.Path("../data/plots_generated_answers/"),
    )
    parser.add_argument(
        "--output_plots_pct_results_dir",
        type=pathlib.Path,
        default=pathlib.Path("../data/plots_pct_results/"),
    )

    # Details of the model and generation options to be used
    parser.add_argument(
        "--model_id",
        type=str,
        required=True,
        help="id of the model for which we need to create the answers",
    )
    parser.add_argument(
        "--format_to_json",
        action="store_true",
        help="If passed, the answers generated by specifying the output to be in json format will be used.",
    )

    # Should be left to default
    parser.add_argument(
        "--label_fixes_path",
        type=pathlib.Path,
        default=pathlib.Path("../data/label_fixes_wright.json"),
        help="Path to the JSON file containing the label fixes if available",
    )

    # Options for testing
    parser.add_argument(
        "--generated_answers_plots_only",
        action="store_true",
        help="If passed, only the plots for the generated answers will be created",
    )
    parser.add_argument(
        "--pct_results_plots_only",
        action="store_true",
        help="If passed, only the plots for the PCT results will be created",
    )
    return parser.parse_args()


def check_create_dir(directory: pathlib.Path):
    if directory.exists():
        logging.info(f"Directory {directory} for storing plots already exists")
    else:
        os.makedirs(directory, exist_ok=True)
        logging.info(f"Directory {directory} for storing plots created")


def main():
    args = parse_command_line_args()
    prepare_logger(args)
    model_name = re.match(r".*/(.*)", args.model_id).group(1)
    model_name_json = f"{model_name}{'_json' if args.format_to_json else ''}"

    # Read generated answers
    input_generate_answers_dir = (
        args.processed_generated_answers_dir / f"{model_name_json}.csv"
    )
    generated_answers_df = pd.read_csv(input_generate_answers_dir)
    generated_answers_df.fillna("None", inplace=True)
    # Change valid -> invalid whenever the evaluator model contains None.
    generated_answers_df['valid'] = ['invalid' if generated_answers_df['decision'][idx] == 'None' else 'valid' for idx in generated_answers_df.index]

    # Add proposition_id and prompt_id to the generated answers df
    proposition_to_id = {prop: idx for idx, prop in enumerate(generated_answers_df['proposition'].unique())}
    prompt_to_id = {prop: idx for idx, prop in enumerate(generated_answers_df['prompt'].unique())}
    generated_answers_df['proposition_id'] = generated_answers_df['proposition'].map(proposition_to_id)
    generated_answers_df['prompt_id'] = generated_answers_df['prompt'].map(prompt_to_id)
    logging.info(
        f"Succesfully read generated answers from {input_generate_answers_dir}"
    )
    # Fix labels from the generated answers.
    label_fixes = read_json(args.label_fixes_path)
    # Clean up the decisions using mappings provided by Wright.
    cnt = 0
    for idx, value in enumerate(generated_answers_df["decision"].values):
        if value not in ANSWER_MAP.keys():
            cnt += 1
            fixed_label = fix_label(value, label_fixes)
            generated_answers_df.loc[idx, "decision"] = fixed_label
    logging.info(f"Fixed labels for {cnt} decisions")
    # Read PCT results
    input_pct_results_dir = (
        args.pct_results_dir / "pct_results.csv"
    )
    pct_results_df = pd.read_csv(input_pct_results_dir)
    input_pct_results_adjusted_dir = (
        args.pct_results_dir / "pct_results_adjusted.csv"
    )
    pct_results_df = pd.read_csv(input_pct_results_dir)
    pct_results_adjusted_df = pd.read_csv(input_pct_results_adjusted_dir)
    logging.info(f"Succesfully read pct results from {input_pct_results_dir}")
    # Filter pct results for the model
    pct_results_df = pct_results_df[pct_results_df["model_id"] == model_name_json]
    pct_results_adjusted_df = pct_results_adjusted_df[pct_results_adjusted_df["model_id"] == model_name_json]
    unique_prompts = pct_results_df['prompt'].unique()
    pct_results_df['prompt_id'] = [idx for idx in range(len(unique_prompts)) for _ in range(len(pct_results_df[pct_results_df['prompt'] == unique_prompts[idx]]))]
    unique_prompts = pct_results_adjusted_df['prompt'].unique()
    pct_results_adjusted_df['prompt_id'] = [idx for idx in range(len(unique_prompts)) for _ in range(len(pct_results_adjusted_df[pct_results_adjusted_df['prompt'] == unique_prompts[idx]]))]
    logging.info(f"Filtered pct results for model {model_name_json}")

    # Create output directories if they do not exist.
    output_generate_answers_plots_dir = (
        args.output_plots_generated_answers_dir / model_name_json
    )
    output_pct_results_plots_dir = args.output_plots_pct_results_dir / model_name_json
    output_pct_results_adjusted_plots_dir = args.output_plots_pct_results_dir / f"{model_name_json}_adjusted"
    check_create_dir(output_generate_answers_plots_dir)
    check_create_dir(output_pct_results_plots_dir)
    check_create_dir(output_pct_results_adjusted_plots_dir)

    # Obtain the plots for the generated answers:
    if args.pct_results_plots_only:
        pass
    else:
        generated_answers_plots(generated_answers_df, output_generate_answers_plots_dir)

    # Obtain the plots for the PCT results (for both adjusted and unadjusted scores):
    if args.generated_answers_plots_only:
        pass
    else:
        pct_results_plot(pct_results_df, output_pct_results_plots_dir)
        pct_results_plot(pct_results_adjusted_df, output_pct_results_adjusted_plots_dir)

def pct_results_plot(df: pd.DataFrame, output_dir: pathlib.Path):
    for additional_context_key_start in ADDITIONAL_CONTEXT_KEY_CATEGORIES:
        plot_context_placement(df,
                               context_key_start=additional_context_key_start,
                               hue_prompt=True,
                               output_plot_path=output_dir / f'additional_context_key_{additional_context_key_start}.png')
        logging.info(f"Succesfully plotted PCT results for context {additional_context_key_start}. Saving it to {output_dir / f'additional_context_key_{additional_context_key_start}.png'}")

def generated_answers_plots(df: pd.DataFrame, output_dir: pathlib.Path):
    
    # Plot shaded bars with valid/invalid responses when using various context keys.
    plot_shaded_bars(
        df,
        "additional_context_key",
        "valid",
        decision_map=VALID_INVALID_MAP,
        output_plot_path=output_dir / 'invalid_by_additional_context_key.png',
    )
    logging.info(
        f"Succesfully plotted invalid responses by additional context key. Saving it to {output_dir / 'invalid_by_additional_context_key.png'}"
    )

    # Plot shaded bars with valid/invalid responses when using various prompt templates.
    plot_shaded_bars(
        df,
        'prompt', 'valid',
        short_column_a_chars=20,
        decision_map=VALID_INVALID_MAP,
        output_plot_path=output_dir / 'invalid_by_prompt.png',
    )
    logging.info(f"Succesfully plotted invalid responses by prompt. Saving it to {output_dir / 'invalid_by_prompt.png'}")

    # Plot shaded bars with valid invalid responses when using various context positions.
    plot_shaded_bars(df,
                     'additional_context_placement',
                     'valid',
                     decision_map=VALID_INVALID_MAP,
                     output_plot_path=output_dir / 'invalid_by_additional_context_placement.png'
                     )
    logging.info(f"Succesfully plotted invalid responses by additional context placement. Saving it to {output_dir / 'invalid_by_additional_context_placement.png'}")

    # Plot shaded bars with distribution of decisions according to the various contexts.
    plot_shaded_bars(df,
                     'additional_context_key',
                     'decision',
                     decision_map=ANSWER_MAP_PLOTTING,
                     color_map=COLOR_MAP,
                     output_plot_path=output_dir / 'decision_by_additional_context_key.png'
                     )
    logging.info(f"Succesfully plotted decisions by additional context key. Saving it to {output_dir / 'decision_by_additional_context_key.png'}")

    # Plots about prompt robustness
    # One plot for each additional_context_key -> Aggregate across various context placements.
    for additional_context_key in df['additional_context_key'].unique():
        plot_shaded_bars(df[(df['additional_context_key'] == additional_context_key)].copy(),
                         'proposition_id', 'decision', short_column_a_chars=None, decision_map=ANSWER_MAP_PLOTTING,
                         color_map=COLOR_MAP, remove_none=False,
                         output_plot_path=output_dir / f'decision_by_proposition_id_{additional_context_key}.png'
                         )
        logging.info(f"Succesfully plotted decisions by proposition id for {additional_context_key}. Saving it to {output_dir / f'decision_by_proposition_id_{additional_context_key}.png'}")
    
    # One plot containing 4 plots showing the results for the 4 additional_context_keys.
    for additional_context_key in df["additional_context_key"].unique():
        # None does not have placement so ignore it.
        if additional_context_key == 'None':
            continue
        create_context_placement_grid(df, ANSWER_MAP_PLOTTING, additional_context_key, COLOR_MAP, output_plot_path=output_dir / f'context_placement_grid_{additional_context_key}.png')
        logging.info(f"Succesfully plotted decision by proposition id with {additional_context_key} across various context placements. Saving it to {output_dir / f'context_placement_grid_{additional_context_key}.png'}")


if __name__ == "__main__":
    main()
