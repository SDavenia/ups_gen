# ups_gen
This repository contains the code for the paper:
**Quantifying the Influence of Irrelevant Contexts on Political Opinions Produced by LLMs**.

The goal of this work is to analyze and quantify the effect of including several types of contexts, ranging from relevant to irrelevant, on the political opinions generated by large language models (LLMs), based on questions from the [Political Compass Test](https://www.politicalcompass.org/) (PCT).


## Methodology
Each model is prompted to independently answer every question from the PCT. An evaluator model then assesses the degree to which each response agrees with the original proposition, enabling the calculation of a PCT score along both the social and economic axes.

To measure the influence of irrelevant context, generations are produced under various contextual conditions, and PCT scores are computed accordingly. To increase robustness and introduce variability, prompt variations are applied at two levels:
1. Question Templates ‚Äì Different formulations are used to prompt the model's opinion (e.g., "You are writing a blog post on <proposition>" vs. "You are at a political rally and must state your opinion on <proposition>").

2. Jailbreak Options ‚Äì Variations are introduced in the phrasing to encourage the model to take a clear and assertive stance on the given proposition.

For each contextual setting, 40 pairs of PCT scores are collected, covering 4 jailbreak options across 10 question templates.

## ‚öôÔ∏è How to run the Pipeline
The code to obtain the generations and the evaluations is briefly described below.

### 1. `generate_answers.py`

Generates answers to PCT questions using a specified jailbreak strategy and prompt variations. Results are saved to `data/generation` in the format:  
`<model_name>.csv`

**Usage:**
```
python generate_answers.py \
  --model_id <HF_MODEL_ID> \
  [--batch_size <BATCH_SIZE>]
```

Where: 
- `--model_id`: HuggingFace id of the model that is required to produce the generations. 

### 2. `wright_open_to_close.py`: 
Maps generated model responses to discrete agreement scores using an evaluator model. Adds a decision column to the input CSV and stores the results in `data/generation_processed` in the format: `<model_name>.csv`. By default (as in [2]), the evaluator is `Mistral-Instruct-7B-v0.3`.

**Usage**:
```
python wright_open_to_close.py \
  --model_data_id <HF_MODEL_ID> \
  [--batch_size <JAILBREAK_ID>] \
```
Where: 
- `--model_data_id`: HuggingFace id of the model that was used to produce the answers.

### 3. `map_to_pct_axis.py`
Maps the set of answers obtained for each specific (`additional_context`, `prompt_template`, `jailbreak_option`) and appends them to `data/results_pct/pct_results.csv`
Additionally, the model decisions are post-processed using hand-crafted rules by [2] which can be found in `/data/label_fixes_wright.json`.

**Usage**:
```
python map_to_pct_axis.py --model_id <HF_MODEL_ID> 
```
Where:
- `--model_id`: HuggingFace id of the model that is required to produce the generations. 


## üìä Plotting & Analysis
Use the following notebooks for visualization and statistical analysis:
- `visualize_pct_shifts.ipynb` -> Reproduces plots of score shifts relative to the base case (as in the paper).
- `visualize_fit_lmm.ipynb` -> Fits linear mixed models for analyzing RQ1 and RQ2.
 
## üìÅ Structure of the Repository
- `src/`  
  - `generate_answers.py` ‚Äì Script for generating answers  
  - `wright_open_to_close.py` ‚Äì Script for obtaining the evaluator model decisions.
  - `map_to_pct_axis.py` ‚Äì Maps decision to PCT.
  - `visualize_pct_shifts.ipynb` ‚Äì Contains code to obtain the plots of the shifts caused by the additional contexts compared to the base case which are contained in the paper.
  - `visualize_fit_lmm.ipynb` ‚Äì Contains code to obtain the statistical analysis contained in the paper.
  - `utils/`  
    - `data.py` ‚Äì Contains functions to create the inputs to the models.
    - `run.py` ‚Äì Contains functions to obtain the model generations.
    - `utils.py` ‚Äì Contains shared utility functions used throughout the project. 
  - `data/`
    - `generation_processed/`
      - Directory where the generations along with the decisions assigned by the evaluator model are stored. Contains all generations as `<MODEL_NAME>.csv` files.
    - `label_fixes_wright.json` ‚Äì Contains the manually crafted fixes for the labels generated by the evaluator model from [2]. 
    - `political_compass/`
      - `political_compass_questions.txt` ‚Äì Contains the full set of PCT questions.
    - `prompting/`
      - `additional_context.json` ‚Äì Contains the Wikipedia paragraphs extracted to be used as additional context.
      - `prompts_wright.json` ‚Äì Contains the generation templates which are included in the model input.
      - `jailbreak_options_rottger.json` ‚Äì Contains the jailbreak options which are included in the model input.
      - `generation_args_wright.json` ‚Äì Contains the args passed to the model for generation (same as [2]).
      - `evaluation_args_wright.json` ‚Äì Contains the args passed to the evaluator model for making the decision (same as [2]).
    - `harmful_content/`
      - `extract_hs.ipynb` ‚Äì Notebook containing the code to run the HS classifier across the various generations and save the results.
      - `visualize_hs_results.ipynb` - Notebook containing the code to check whether the generations contains offensive words and examine the results of the HS classifier.
      - `davidson_hate_words.txt` - List of highly offensive words inspired by the lexicon from [3].


## References
- [1] Paul R√∂ttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Kirk, Hinrich Sch√ºtze, and Dirk Hovy. (2024). **Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.** *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 15295‚Äì15311, Bangkok, Thailand.

- [2] Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, and Isabelle Augenstein. (2024). **LLM Tropes: Revealing Fine-Grained Values and Opinions in Large Language Models.**  *Findings of the Association for Computational Linguistics: EMNLP 2024*, pp. 17085‚Äì17112, Miami, Florida, USA.  Association for Computational Linguistics.

- [3] Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. (2017). **Automated Hate Speech Detection and the Problem of Offensive Language.** *Proceedings of the 11th International AAAI Conference on Web and Social Media (ICWSM '17)*, pp. 512‚Äì515, Montreal, Canada.

