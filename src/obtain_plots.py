import pathlib
import os
import re
import argparse
import logging
import copy

import pandas as pd
import matplotlib.pyplot as plt

from copy import deepcopy as dp
from utils.utils import prepare_logger, fix_label, read_json
from utils.plotting_utils import plot_shaded_bars, create_context_placement_grid, political_compass_base_plot_on_ax, add_datapoints
from utils.plotting_utils import plot_context_placement


JAILBREAK_OPTION = 'jail-03'
ADDITIONAL_CONTEXT_PLACEMENT = 'user-beginning'

VALID_INVALID_MAP = {"valid": 1, "invalid": 0, "neutral": 0.5}  # Neutral is considered invalid for sorting from most to least amount of answers.
ANSWER_MAP = {
    "Strongly disagree": 0,
    "Strongly Disagree": 0,
    "Disagree": 1,
    "disagree": 1,
    "Agree": 2,
    "agree": 2,
    "Strongly agree": 3,
    "Strongly Agree": 3,
    "None": -1,
}
ANSWER_MAP_PLOTTING = ANSWER_MAP.copy()  # None has to be in between
ANSWER_MAP_PLOTTING["None"] = 1.5
ADDITIONAL_CONTEXT_KEY_CATEGORIES = ['base', 'wiki_mus', 'wiki_obj', 'wiki_pol']

COLOR_MAP = {
    "Strongly Disagree": "#8B0000",  # Dark Red (DarkRed)
    "Disagree": "#FF6347",  # Light Red (Tomato)
    "None": "#AAAAAA",  # Gray
    "Agree": "#90EE90",  # Light Gree
    "Strongly Agree": "#006400",  # Dark Green (DarkGreen)
}

def parse_command_line_args():
    parser = argparse.ArgumentParser()
    # General directories to read dataframes from: should be left to default structure of the directory.
    parser.add_argument(
        "--processed_generated_answers_dir",
        type=pathlib.Path,
        default=pathlib.Path("../data/generation_processed/"),
        help="Directory containing the answers generated by the model",
    )
    parser.add_argument(
        "--pct_results_dir",
        type=pathlib.Path,
        default=pathlib.Path("../data/results_pct/"),
    )
    # General directories to write dataframes to
    parser.add_argument(
        "--output_plots_generated_answers_dir",
        type=pathlib.Path,
        default=pathlib.Path("../data/plots_generated_answers/specific/"),
    )
    parser.add_argument(
        "--output_plots_pct_results_dir",
        type=pathlib.Path,
        default=pathlib.Path("../data/plots_pct_results/specific/"),
    )

    # Details of the model and generation options to be used
    parser.add_argument(
        "--model_id",
        type=str,
        required=True,
        help="id of the model for which we need to create the answers",
    )
    parser.add_argument(
        "--format_to_json",
        action="store_true",
        help="If passed, the answers generated by specifying the output to be in json format will be used.",
    )

    # Should be left to default
    parser.add_argument(
        "--label_fixes_path",
        type=pathlib.Path,
        default=pathlib.Path("../data/label_fixes_wright.json"),
        help="Path to the JSON file containing the label fixes if available",
    )

    # Options for testing
    parser.add_argument(
        "--generated_answers_plots_only",
        action="store_true",
        help="If passed, only the plots for the generated answers will be created",
    )
    parser.add_argument(
        "--pct_results_plots_only",
        action="store_true",
        help="If passed, only the plots for the PCT results will be created",
    )
    return parser.parse_args()


def check_create_dir(directory: pathlib.Path):
    if directory.exists():
        logging.info(f"Directory {directory} for storing plots already exists")
    else:
        os.makedirs(directory, exist_ok=True)
        logging.info(f"Directory {directory} for storing plots created")


def main():
    args = parse_command_line_args()
    prepare_logger(args)
    model_name = re.match(r".*/(.*)", args.model_id).group(1)
    model_name_json = f"{model_name}{'_json' if args.format_to_json else ''}"

    # Read generated answers
    input_generate_answers_dir = (
        args.processed_generated_answers_dir / f"{model_name_json}.csv"
    )
    generated_answers_df = pd.read_csv(input_generate_answers_dir)
    generated_answers_df.fillna("None", inplace=True)

    # Add proposition_id and prompt_id to the generated answers df
    proposition_to_id = {prop: idx for idx, prop in enumerate(generated_answers_df['proposition'].unique())}
    prompt_to_id = {prop: idx for idx, prop in enumerate(generated_answers_df['prompt'].unique())}
    generated_answers_df['proposition_id'] = generated_answers_df['proposition'].map(proposition_to_id)
    generated_answers_df['prompt_id'] = generated_answers_df['prompt'].map(prompt_to_id)
    logging.info(
        f"Succesfully read generated answers from {input_generate_answers_dir}"
    )
    # Fix labels from the generated answers.
    label_fixes = read_json(args.label_fixes_path)
    # Clean up the decisions using mappings provided by Wright.
    cnt = 0
    for idx, value in enumerate(generated_answers_df["decision"].values):
        if value not in ANSWER_MAP.keys():
            cnt += 1
            fixed_label = fix_label(value, label_fixes)
            generated_answers_df.loc[idx, "decision"] = fixed_label
    logging.info(f"Fixed labels for {cnt} decisions")
    # Read PCT results
    input_pct_results_dir = (
        args.pct_results_dir / "pct_results.csv"
    )
    pct_results_df = pd.read_csv(input_pct_results_dir)
    input_pct_results_adjusted_dir = (
        args.pct_results_dir / "pct_results_adjusted.csv"
    )
    pct_results_df = pd.read_csv(input_pct_results_dir)
    pct_results_adjusted_df = pd.read_csv(input_pct_results_adjusted_dir)
    logging.info(f"Succesfully read pct results from {input_pct_results_dir}")
    # Filter pct results for the model
    pct_results_df = pct_results_df[pct_results_df["model_id"] == model_name_json]
    pct_results_adjusted_df = pct_results_adjusted_df[pct_results_adjusted_df["model_id"] == model_name_json]
    unique_prompts = pct_results_df['prompt'].unique()
    pct_results_df['prompt_id'] = [idx for idx in range(len(unique_prompts)) for _ in range(len(pct_results_df[pct_results_df['prompt'] == unique_prompts[idx]]))]
    unique_prompts = pct_results_adjusted_df['prompt'].unique()
    pct_results_adjusted_df['prompt_id'] = [idx for idx in range(len(unique_prompts)) for _ in range(len(pct_results_adjusted_df[pct_results_adjusted_df['prompt'] == unique_prompts[idx]]))]
    logging.info(f"Filtered pct results for model {model_name_json}")

    # Create output directories if they do not exist.
    output_generate_answers_plots_dir = (
        args.output_plots_generated_answers_dir / model_name_json
    )
    output_pct_results_plots_dir = args.output_plots_pct_results_dir / model_name_json
    output_pct_results_adjusted_plots_dir = args.output_plots_pct_results_dir / f"{model_name_json}_adjusted"
    check_create_dir(output_generate_answers_plots_dir)
    check_create_dir(output_pct_results_plots_dir)
    check_create_dir(output_pct_results_adjusted_plots_dir)

    # Do the specific filtering
    generated_answers_df = generated_answers_df[(generated_answers_df['additional_context_placement'] == ADDITIONAL_CONTEXT_PLACEMENT) & (generated_answers_df['jailbreak_option'] == JAILBREAK_OPTION)]
    pct_results_df = pct_results_df[((pct_results_df['additional_context_placement'] == ADDITIONAL_CONTEXT_PLACEMENT) | (pct_results_df['additional_context_placement'] == 'base')) & (pct_results_df['jailbreak_option'] == JAILBREAK_OPTION)]
    pct_results_adjusted_df = pct_results_adjusted_df[((pct_results_adjusted_df['additional_context_placement'] == ADDITIONAL_CONTEXT_PLACEMENT) | (pct_results_adjusted_df['additional_context_placement'] == 'base')) & (pct_results_adjusted_df['jailbreak_option'] == JAILBREAK_OPTION)]
    # Create plots for generated answers.
    # Distribution of answers by additional_context_key
    plot_shaded_bars(generated_answers_df,
                    'additional_context_key',
                    'decision',
                    decision_map=ANSWER_MAP_PLOTTING,
                    color_map=COLOR_MAP,
                    output_plot_path=args.output_plots_generated_answers_dir / model_name / 'decision_by_additional_context_key.png'
                    )
    logging.info(f"Succesfully plotted decisions by additional context key. Saving it to {args.output_plots_generated_answers_dir / 'decision_by_additional_context_key.png'}")
    # Distribution of answers by prompt template
    plot_shaded_bars(generated_answers_df,
                    'prompt',
                    'decision',
                    short_column_a_chars=20,
                    decision_map=ANSWER_MAP_PLOTTING,
                    color_map=COLOR_MAP,
                    output_plot_path=args.output_plots_generated_answers_dir / model_name / 'decision_by_prompt_id.png'
                    )
    # PCT results plots -> For each additional_context_key_type (base, pol, mus, obj) -> Obtain a PCT plot with the all the individual ones.
    # Create each subplot
    for context_key_start in ADDITIONAL_CONTEXT_KEY_CATEGORIES:
        fig, axes = plt.subplots(1, 2, figsize=(12, 12))
        # Flatten axes for easier iteration
        axes = axes.flatten()

        df_filtered = pct_results_df[(pct_results_df['additional_context_key'].str.startswith(context_key_start))].copy()
        ax = political_compass_base_plot_on_ax(axes[0])
        add_datapoints(ax, df_filtered, hue_col='prompt_id', style_col='additional_context_key')
        # Set title for each subplot
        ax.set_title(f"PCT Results - Raw Scores")

        df_filtered_adjusted = pct_results_adjusted_df[(pct_results_adjusted_df['additional_context_key'].str.startswith(context_key_start))].copy()
        ax = political_compass_base_plot_on_ax(axes[1])
        add_datapoints(ax, df_filtered_adjusted, hue_col='prompt_id', style_col='additional_context_key')
        # Set title for each subplot
        ax.set_title(f"PCT Results - Adjusted Scores")
        plt.tight_layout()

        # Add a main title for the entire figure
        titles_context_key_str = {
        'wiki_mus': 'Wikipedia Music',
        'wiki_obj': 'Wikipedia Object',
        'wiki_pol': 'Wikipedia Political Person',
        'base': 'Base',
        }
        fig.suptitle(f"PCT Results with {titles_context_key_str[context_key_start]} and Various Prompt IDs",
                    fontsize=16, y=1.02)
        plt.savefig(args.output_plots_pct_results_dir / model_name / f"pct_results_{context_key_start}.png")
        logging.info(f"Saved PCT results plot for {context_key_start} to {args.output_plots_pct_results_dir / model_name  / f'pct_results_{context_key_start}.png'}")

if __name__ == '__main__':
    main()

